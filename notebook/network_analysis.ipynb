{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from datetime import datetime as dat\n",
    "import jdatetime\n",
    "from jdatetime import datetime as jdt\n",
    "from numpy import random as nprnd\n",
    "import persiantools\n",
    "from persiantools import characters\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from networkx import NetworkXError\n",
    "from networkx import bipartite\n",
    "import networkx.algorithms.connectivity as nxcon\n",
    "from networkx.algorithms import approximation as nxp\n",
    "import networkx.algorithms.community as nxcom\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import pyodbc\n",
    "from difflib import SequenceMatcher\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_csv(path,dirr):\n",
    "    extention='xlsx'\n",
    "    os.chdir(path)\n",
    "    for file in os.listdir(path):\n",
    "        file_xlsx= pd.read_ecxel(file)\n",
    "        file_xlsx.to_csv(dirr+\"\\\\\"+file.split('.')[0]+\".csv\",index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_xlsx(path,dirr):\n",
    "    extention='csv'\n",
    "    os.chdir(path)\n",
    "    for file in os.listdir(path):\n",
    "        file_xlsx= pd.read_csv(file)\n",
    "        file_xlsx.to_excel(dirr+\"\\\\\"+file.split('.')[0]+\".xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merge_output(path,dirr=None):\n",
    "    input_1=input('select the type of file: (csv/xlsx) ')\n",
    "    all_df=[]\n",
    "    if input_1=='xlsx':\n",
    "        input_2=input('insert the number of columns: ')\n",
    "        input_2= int(input_2)\n",
    "        for file in os.listdir(path):\n",
    "            df=pd.read_xlsx(path+'\\\\'+file)\n",
    "            file_name= file.split('.')[0]\n",
    "            column_list= df.columns.tolist()\n",
    "            len_df= len(column_list)\n",
    "            if len_df== input_2:\n",
    "                all_df.append(df)\n",
    "            else:\n",
    "                print(f'the number of columns of file {file_name} is {len_df}')\n",
    "                input_3= input('remove column or add column: (add/remove) ')\n",
    "                if input_3 == 'add':\n",
    "                    input_4= input('name of column: ')\n",
    "                    df[input_4]=None\n",
    "                    all_df.append(df)\n",
    "                if input_3=='remove':\n",
    "                    input_5= input('name of removed column: ')\n",
    "                    df.drop(columns=input_5)\n",
    "                    all_df.append(df)\n",
    "    if input_1=='csv':\n",
    "        input_2=input('insert the number of columns: ')\n",
    "        input_2= int(input_2)\n",
    "        for file in os.listdir(path):\n",
    "            df=pd.read_csv(path+'\\\\'+file)\n",
    "            file_name= file.split('.')[0]\n",
    "            column_list= df.columns.tolist()\n",
    "            len_df= len(column_list)\n",
    "            if len_df== input_2:\n",
    "                all_df.append(df)\n",
    "            else:\n",
    "                print(f'the number of columns of file {file_name} is {len_df}')\n",
    "                input_3= input('remove column or add column: (add/remove) ')\n",
    "                if input_3 == 'add':\n",
    "                    input_4= input('name of column: ')\n",
    "                    df[input_4]=None\n",
    "                    all_df.append(df)\n",
    "                if input_3=='remove':\n",
    "                    input_5= input('name of removed column: ')\n",
    "                    df.drop(columns=input_5)\n",
    "                    all_df.append(df)\n",
    "    input_6=input('do you want output: (y/n) ')\n",
    "    if input_6=='y':\n",
    "        df_raw= pd.concat(all_df,ignore_index=True).reset_index(drop=True)\n",
    "        input_7=input('file type: (xlsx\\csv) ')\n",
    "        if input_7== 'xlsx':\n",
    "            input_8= ('file name: ')\n",
    "            df_raw.to_excel(dirr+'\\\\'+input_8+'.xlsx',index=False)\n",
    "        if input_7== 'csv':\n",
    "            input_8= ('file name: ')\n",
    "            df_raw.to_csv(dirr+'\\\\'+input_8+'.csv',index=False)\n",
    "    if input_6=='n':\n",
    "        df_raw= pd.concat(all_df,ignore_index=True).reset_index(drop=True)\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_name(df):\n",
    "    columns= df.columns.tolist()\n",
    "    print(f'column name is: {columns}')\n",
    "    input_1= input('do you want to change header: (n\\y)')\n",
    "    if input_1=='y':\n",
    "        for i in columns:\n",
    "            input_2= input(f'insert new name for {i}: ')\n",
    "            df.rename(columns={i:input_2},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_clustering(df):\n",
    "    columns= df.columns.tolist()\n",
    "    print(f'column name is: {columns}')\n",
    "    string_columns=[]\n",
    "    code_columns=[]\n",
    "    price_columns=[]\n",
    "    input_1=None\n",
    "    while input_1!='finish':\n",
    "        input_1= input('choose type of clusters: (string\\price\\code) ')\n",
    "        if input_1=='string':\n",
    "            input_2=None\n",
    "            while input_2!='finish':\n",
    "                input_2=input('choose string column')\n",
    "                string_columns.append(input_2)\n",
    "            if 'finish' in string_columns:\n",
    "                string_columns.remove('finish')\n",
    "        if input_1=='code':\n",
    "            input_2=None\n",
    "            while input_2!='finish':\n",
    "                input_2=input('choose code column')\n",
    "                code_columns.append(input_2)\n",
    "            if 'finish' in code_columns:\n",
    "                code_columns.remove('finish')\n",
    "        if input_1=='price':\n",
    "            input_2=None\n",
    "            while input_2!='finish':\n",
    "                input_2=input('choose price column')\n",
    "                price_columns.append(input_2)\n",
    "            if 'finish' in price_columns:\n",
    "                price_columns.remove('finish')\n",
    "\n",
    "    return string_columns,code_columns,price_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_clean(df, code_columns):\n",
    "    for i in code_columns:\n",
    "        try:\n",
    "            df[i]= df[i].replace(['nan',np.nan,'NaN'],1000000000001234)\n",
    "            df[i]=df[i].replace(r'-','',regex=True)\n",
    "            df[i]=df[i].apply(np.int64)\n",
    "            df[i]= df[i].astype(str)\n",
    "            df[i]= df[i].replace(r'1000000000001234',np.nan, regex=True)\n",
    "        except:\n",
    "            print(f'error is in {i}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_clean (df, price_columns):\n",
    "    input_1= input('type of price: (float/int)')\n",
    "    if input_1=='int':\n",
    "        for i in code_columns:\n",
    "            try:\n",
    "                df[i]= df[i].apply(lambda x: x.astype(str))\n",
    "                df[i]=df[i].apply(lambda x: x(np.int64))\n",
    "            except:\n",
    "                print(f'error is in {i}')\n",
    "    if input_1=='float':\n",
    "        for i in code_columns:\n",
    "            try:\n",
    "                df[i]= df[i].apply(lambda x: x.astype(float))\n",
    "            except:\n",
    "                print(f'error is in {i}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_clean(df,string_columns):\n",
    "    for i in string_columns:\n",
    "        try:\n",
    "            df[i]= df[i].apply(lambda x: x.astype(str))\n",
    "            df[i]=df[i].str.lower()\n",
    "            df[i]=df[i].str.strip()\n",
    "            df[i]=df[i].replace('  ',' ')\n",
    "            df[i]=df[i].replace('(','-')\n",
    "            df[i]=df[i].replace(')','')\n",
    "            df[i]=df[i].replace('_',' ')\n",
    "            df[i]=df[i].apply(lambda x: characters.ar_to_fa(x))\n",
    "            df[i]=df[i].replace(regex=r'\\u200c',value=' ')\n",
    "            df[i]=df[i].str.strip(' ')\n",
    "            df[i]=df[i].replace(regex=['مجتمع','سهامی خاص','شرکت','سهامی عام'],value='')\n",
    "        except:\n",
    "            print(f'error is in {i}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_E (df):\n",
    "    def unique_value(df,column):\n",
    "        x= df[column].drop_duplicates()\n",
    "        return x\n",
    "    columns= df.columns.tolist()\n",
    "    print(f'list of columns: {columns}')\n",
    "    columns_list=[]\n",
    "    input_1=None\n",
    "    while input_1 != 'finish':\n",
    "        input_1= input('select column: ')\n",
    "        columns_list.append(input_1)\n",
    "    if 'finish' in columns_list:\n",
    "        columns_list.remove('finish')\n",
    "    for j in columns_list:\n",
    "        u = unique_value(df,[j])\n",
    "        input_2= input(f'{j} in your dataframe has {len(u)} unique value; do you want to replace: (N/Y)')\n",
    "        if input_2=='n':\n",
    "            continue\n",
    "        else:\n",
    "            dict_replace={}\n",
    "            c=1\n",
    "            for i in u.iloc[:,0]:\n",
    "                input_3= input(f'{c} what is replace of {i}')\n",
    "                dict_replace[i]= input_3\n",
    "                if input_3 =='break':\n",
    "                    break\n",
    "                else:\n",
    "                    c+=1\n",
    "        input_4= input('do you want creat new column?')\n",
    "        if input_4 == 'break':\n",
    "            break\n",
    "        else:\n",
    "            if input_4=='y':\n",
    "                name=''\n",
    "                input_5= input('name of new column: ')\n",
    "                df[f'{input_5}']= df[j].replace(dict_replace)\n",
    "            else:\n",
    "                df[j]= df[j].replace(dict_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_clean(df,columns_list:list):\n",
    "    for j in columns_list:\n",
    "        v= df[j].value_counts()\n",
    "        input_1= input(f'{j} has: {v}/n do wou want to clean: (n/y)')\n",
    "        if input_1=='n':\n",
    "            continue\n",
    "        else:\n",
    "            list_replace=[]\n",
    "            input_2=None\n",
    "            while input_2 != 'finish':\n",
    "                input_2= input ('what is the words to remove: ')\n",
    "                list_replace.append(input_2)\n",
    "            if 'finish' in list_replace:\n",
    "                list_replace.remove('finish')\n",
    "            input_3= input('do you want to creat new column: ')\n",
    "            if input_3 =='break':\n",
    "                break\n",
    "            else:\n",
    "                if input_3=='y':\n",
    "                    name=''\n",
    "                    input_4= input('new column name: ')\n",
    "                    df[f'{input_4}']= df[j].replace(regex=list_replace,value='')\n",
    "                else:\n",
    "                    df[j]= df[j].replace(regex=list_replace,value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_words (pd_series):\n",
    "    pd_series= pd_series.astype(str)\n",
    "    pd_series= pd_series.replace(r'سهامی عام','',regex=True)\n",
    "    pd_series= pd_series.replace(r'شرکت','',regex=True)\n",
    "    pd_series= pd_series.replace(r'مجتمع','',regex=True)\n",
    "    pd_series= pd_series.replace(r'سهامی خاص','',regex=True)\n",
    "    pd_series= pd_series.str.strip()\n",
    "    pd_series= pd_series.str.strip(' ')\n",
    "    return pd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_space (pd_series):\n",
    "    pd_series=pd_series.astype(str)\n",
    "    pd_series=pd_series.str.strip(' ')\n",
    "    return pd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(pd_series):\n",
    "    pd_series= pd_series.astype(str)\n",
    "    pd_series.replace(r' ','',regex=True)\n",
    "    return pd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_timestamp(pd_series):\n",
    "    pd_series=pd_series.astype(str)\n",
    "    date = pd_series.apply (lambda x: str(x).split(' ')[0])\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_corrector(pd_series):\n",
    "    x= pd_series.astype(str)\n",
    "    year= x.apply(lambda x: str(x)[0:4])\n",
    "    month= x.apply(lambda x: str(x)[4:6])\n",
    "    day= x.apply(lambda x: str(x)[6:])\n",
    "    date = year +'-'+month+'-'+day\n",
    "    pd_series= date\n",
    "    pd_series= pd_series.replace(regex='<NA>--', value= np.nan)\n",
    "    return pd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_value_str(df):\n",
    "    y = df.columns.tolist()\n",
    "    print('Note: it takes case sensitive values.')\n",
    "    keyword= input('keyword for search')\n",
    "    dict_name={}\n",
    "    for k in y:\n",
    "        try:\n",
    "            l= df[df[k].str.contains(keyword)]\n",
    "            dict_name[k]= l.index.tolist()\n",
    "        except:\n",
    "            continue\n",
    "    print(dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(df):\n",
    "    css = \"\"\"<style>\n",
    "    table {border-collapse: collapse; border: 3px solid #eee; }\n",
    "    table tr th: first-child {background-color: #eee; color: #000; }\n",
    "    table thead th {background-color: #eee; color:#000 }\n",
    "    tr, th, td {border: 1px solid #ccc; border-width: 1px 0 0 1px; border-collapse: collapse;\n",
    "    padding: 3px: font-family: monospace; font-size: 10px }<style>\n",
    "    \"\"\"\n",
    "    s= '<script type = \"text/Javascript\">'\n",
    "    s+= 'var win = window.open(\"\", \"Title\",\"toolbar=no\", location=no, directories=no, status=no, menubar=no, scrollbars=yes, resizable=yes, width=780, height=200, top=\"+(screen.height-400+\", left=\"+(screen.width-840\"));'\n",
    "    s+= (\n",
    "        \"win.document.body.innerHTML = '\"\n",
    "        + (df.to_html())+ css).replace(\"\\n\",\"\\\\\")\n",
    "        + \"';\"\n",
    "    )\n",
    "    s+= \"</script>\"\n",
    "    return HTML(s+css)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot (G, dt):\n",
    "    '''Convert date to integer timestamp'''\n",
    "    # dt= datetime.datetime.strptime(date, '%Y-%m-%d)\n",
    "    timestamp= time.mktime(dt.timetuple())\n",
    "\n",
    "    # Find edges that exsisted during timestamp\n",
    "    snapshot_edge=[]\n",
    "    for e in G.edges:\n",
    "        if G.edges[e]['begin']<= timestamp and G.edges[e]['end'] >= timestamp:\n",
    "            snapshot_edges.append(e)\n",
    "    return nx.Graph(G.edge_subgraph(snapshot_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_str (df):\n",
    "    list_similar=[]\n",
    "    name_list=[]\n",
    "    company_name= list(dict.fromkeys(df['company_name']))\n",
    "    person_name= list(dict.fromkeys(df['person_name']))\n",
    "    for i, j in itertools.product(name_list, list(dict.fromkeys(df['person_anme']))):\n",
    "        s= SequenceMatcher(None, j, i).ratio()\n",
    "        if s>= 0.6 and s<1:\n",
    "            list_index= df.loc[df['person_name']==j].index.tolist()\n",
    "            sim= {i : list_index}\n",
    "            list_similar.append(sim)\n",
    "    return list_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_str_map (df,i,j):\n",
    "    list_similar=[]\n",
    "    s= SequenceMatcher(None, j, i).ratio()\n",
    "    if s>= 0.6 and s<1:\n",
    "        list_index= df.loc[df['person_name']==j].index.tolist()\n",
    "        sim= {i : list_index}\n",
    "        list_similar.append(sim)\n",
    "    return list_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_words(df):\n",
    "    '''this function find differences in dictation or code of person name with natioanl code'''\n",
    "    def check(df):\n",
    "        national_id_list= list(dict.fromkeys(df['person_national_code']))\n",
    "        if 'nan' in national_id_list:\n",
    "            national_id_list.remove('nan')\n",
    "        list_for_check=[]\n",
    "        person_with_nan= []\n",
    "        for i in national_id_list:\n",
    "            list_index= df.loc[df['person_national_code']==i].index.tolist()\n",
    "            sub_df= df.loc[list_index]\n",
    "            list_person_name= list(dict.fromkeys(sub_df['person_name']))\n",
    "            if 'nan' in list_person_name:\n",
    "                print(f'there is nan in {i} person name')\n",
    "                person_with_nan.append(i)\n",
    "            if len(list_person_name)>1:\n",
    "                list_for_check.append({i : list_person_name})\n",
    "        return list_for_check\n",
    "    list_for_check=['0']\n",
    "    c=1\n",
    "    while len(list_for_check)!=0:\n",
    "        start_time= datetime.datetime.now()\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'step_{str(c)}')\n",
    "        list_for_check= check(df)\n",
    "        if len(list_for_check) > 0 :\n",
    "            print('there is problem in word dictation or code')\n",
    "            print(list_for_check)\n",
    "            for i in list_for_check:\n",
    "                for key,value in i.items():\n",
    "                    list_index_1= df.loc[df['person_national_code']==key].index.tolist()\n",
    "                    sub_df_1= df.loc[list_index_1]\n",
    "                    list_person_1= list(dict.fromkeys(sub_df_1['person_name']))\n",
    "                    for j in list_person_1:\n",
    "                        input_3= input(f'for {j} : dictation or code or nothing: ')\n",
    "                        if input_3== 'dict' or input_3=='یهزف':\n",
    "                            input_4= input(f'enter new dictation for {i} :')\n",
    "                            df['person_name'] = df['person_name'].replace([j], input_4)\n",
    "                        if input_3=='code' or input_3=='زخیث':\n",
    "                            input_4= input(f'enter new code for {j} :')\n",
    "                            df['person_national_code']= df['person_national_code'].mask(df['person_name']==j, input_4)\n",
    "                        if input_3=='nothing' or input_3=='دخفاهدل':\n",
    "                            continue\n",
    "            c= c+1\n",
    "        end_time= datetime.datetime.now()\n",
    "        print('Duration: {}'.format(end_time - start_time))\n",
    "    print('there is no problem in word dictation or code')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_company_person(df, national_company_series,national_person_series,company_name_series,person_name_series):\n",
    "    \"\"\" checking dictation and code of company name and person name\"\"\"\n",
    "    def ckeck(df, national_company_series,national_person_series,company_name_series,person_name_series):\n",
    "        national_id_list= list(dict.fromkeys(df[national_company_series]))\n",
    "        if 'nan' in national_id_list:\n",
    "            national_id_list.remove('nan')\n",
    "        list_for_check=[]\n",
    "        person_with_nan=[]\n",
    "        company_with_nan=[]\n",
    "        for i in national_id_list:\n",
    "            list_index_company= df.loc[df[national_company_series]==i].index.tolist()\n",
    "            list_index_person= df.loc[df[national_person_series]==i].index.tolist()\n",
    "            sub_df_person= df.loc[list_index_person]\n",
    "            sub_df_company= df.loc[list_index_company]\n",
    "            list_person_name= list(dict.fromkeys(sub_df_pesron[person_name_series]))\n",
    "            list_company_name= list(dict.fromkeys(sub_df_company[company_name_series]))\n",
    "            if 'nan' in list_person_name:\n",
    "                print(f'there is nan in {i} {person_name_series}')\n",
    "                list_person_name.remove('nan')\n",
    "                person_with_nan.append(i)\n",
    "            if 'nan' in list_company_name:\n",
    "                print(f'there is nan in {i} {company_name_series}')\n",
    "                list_company_name.remove('nan')\n",
    "                company_with_nan.append(i)\n",
    "            if len(list_person_name)>1 or len (list_company_name)>1:\n",
    "                for j in list_company_name:\n",
    "                    list_person_name.append(j)\n",
    "                list_person_name= list(dict.fromkeys(list_person_name))\n",
    "                list_for_check.append({i : list_person_name})\n",
    "        return list_for_check\n",
    "    list_for_check=['0']\n",
    "    c = 1\n",
    "    while len(list_for_check)!= 0:\n",
    "        start_time= datetime.datetime.now()\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'step_{str(c)}')\n",
    "        list_for_check= ckeck(df, national_company_series,national_person_series,company_name_series,person_name_series)\n",
    "        if len(list_for_check)>0:\n",
    "            print('there is problem in word dictation or code')\n",
    "            print(list_for_check)\n",
    "            for i in list_for_check:\n",
    "                for key,value in i.items():\n",
    "                    list_index_company= df.loc[df[national_company_series]==key].index.tolist()\n",
    "                    list_index_person= df.loc[df[national_person_series]== key].index.tolist()\n",
    "                    sub_df_company= df.loc[list_index_company]\n",
    "                    suc_df_person= df.loc[list_index_person]\n",
    "                    list_person_name= list(dict.fromkeys(suc_df_person[person_name_series]))\n",
    "                    list_company_name= list(dict.fromkeys(sub_df_company[company_name_series]))\n",
    "                    for x in list_company_name:\n",
    "                        list_person_name.append(x)\n",
    "                    list_person_name= list(dict.fromkeys(list_person_name))\n",
    "                    for j in list_person_name:\n",
    "                        input_3= input(f'for {j} : dictation or code or nathing: ')\n",
    "                        if input_3== 'dict' or input_3== 'یهزف':\n",
    "                            input_4= input(f'enter new dictation for {j}')\n",
    "                            df[company_name_series] = df[company_name_series].replace([j], input_4)\n",
    "                            df[person_name_series] = df[person_name_series].replace([j], input_4)\n",
    "                        if input_3== 'code' or input_3== 'زخیث':\n",
    "                            input_4 = input(f'enter new code for {j}')\n",
    "                            df[national_company_series] = df[national_company_series].mask(df[company_name_series]==j, input_4)\n",
    "                            df[national_person_series] = df[national_person_series].mask(df[person_name_series]==j, input_4)\n",
    "                        if input_3== 'nothing' or 'دخفاهدل':\n",
    "                            continue\n",
    "            c= c+1\n",
    "        end_time= datetime.datetime.now()\n",
    "        print('Duration: {}'.format(end_time - start_time))\n",
    "    print('there is no problem in word dictation or code')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length_histogram(G, title=None):\n",
    "    # find path lengths\n",
    "    length_source_target= dict(nx.shortest_path_length(G))\n",
    "    # convert dict of dicts to flat list\n",
    "    all_shortest = sum([ \n",
    "        list(length_target.values())\n",
    "        for length_target\n",
    "        in length_source_target.value()],\n",
    "    [])\n",
    "    # calculate integer bins\n",
    "    high= max(all_shortest)\n",
    "    bins= [-0.5 + i for i in range (high + 2)]\n",
    "    # plot histogram\n",
    "    plt.hist(all_shortest, bins=bins, rwidth=3)\n",
    "    plt.title(title)\n",
    "    plt.xlable('Distance')\n",
    "    plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_connected(G, u, v):\n",
    "    return u in G.neighbors(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_layers_node (G, start, k):\n",
    "    nbrs= list([start])\n",
    "    for l in range(k):\n",
    "        nbrs= list(nbr for n in nbrs for nbr in G[n])\n",
    "    return nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_code(df, code_column_name, name_column_name):\n",
    "    def checking_dictation(df, person_name):\n",
    "        input_1 = input(f'insert new dictation for {person_name}: ')\n",
    "        list_index_name= df.loc[df['person_name']== person_name].index_tolist()\n",
    "        df.loc[list_index_name , ['person_name']] == [input_1]\n",
    "    nan_person_national_code= df.loc[df[code_column_name]=='nan']\n",
    "    nan_person_name= list(nan_person_national_code[name_column_name])\n",
    "    nan_person_name_drop_duplicate= list(dict.fromkeys(nan_person_name))\n",
    "    nan_person_name_drop_duplicate.sort(reversed=True)\n",
    "    print('len nan_person_name ', len(nan_person_name))\n",
    "    print('len nan_person_name_drop_duplicate ', len(nan_person_name_drop_duplicate))\n",
    "    for i in nan_person_name_drop_duplicate:\n",
    "        list_index= df.loc[df[name_column_name]==i].index.tolist()\n",
    "        input_1= input(f'for {i} : (code/dict/next/done)')\n",
    "        if input_1=='code'or input_1=='زخیث':\n",
    "            input_3= input(f'insert code for {i} :')\n",
    "            df.loc[list_index, [code_column_name]]=[input_3]\n",
    "            input_2= input('Running checking dictation (n/y)')\n",
    "            if input_2=='y' or input_2=='غ':\n",
    "                checking_dictation(df, person_name=i)\n",
    "            if input_2=='n' or input_2=='د':\n",
    "                continue\n",
    "        if input_1=='dict' or 'یهزف':\n",
    "            input_3 = input(f'new dict for {i} :')\n",
    "            df.loc[list_index, [name_column_name]]=[input_3]\n",
    "        if input_1== 'next' or 'دثطف':\n",
    "            continue\n",
    "        if input_1== 'done' or input_1 =='یخدث':\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default measuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams.update({ \n",
    "    'figure.figsize' : (24 , 24),\n",
    "    'axes.spines.right' : False,\n",
    "    'axes.spines.left' : False,\n",
    "    'axes.spines.top' : False,\n",
    "    'axes.spines.bottom' : False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= hash('Network Science in python') %2**32\n",
    "nprnd.seed(seed=seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today= datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server= 'server name'\n",
    "database= 'database name'\n",
    "username= 'username'\n",
    "password= 'your username password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxn = db.connect( \n",
    "    'DRIVER={SQL Server}; SERVER=' +server+';DATABASE=' +database+';UTD'+username+';PWD'+password\n",
    ")\n",
    "cursor= cnxn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_adress= input(r'insert adress of raw file: ')\n",
    "df= read_merge_output(path=f'{file_adress}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_column_name(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_columns= column_clustering(df)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns= column_clustering(df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy= df.copy()\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_copy= code_clean(df_copy, code_columns)\n",
    "df_copy= string_clean(df_copy, string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_file= input('bank name: ')\n",
    "df_copy.to_csv(f'saveing adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_copy.iterrows():\n",
    "    sursor.execute('''\n",
    "                    INSERT INTO table name (column names)\n",
    "                    VALUES (?)\n",
    "                    ''',\n",
    "                    row.loc['name_column']\n",
    "                    )\n",
    "cbxb.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleansing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_index= df_copy.loc[df_copy['agent_name'] != 'nan'].index.tolist()\n",
    "df_index= df_copy.index.tolist()\n",
    "rest_index= list(set(df_index)-set(agent_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(agent_index))\n",
    "print(len(df_index))\n",
    "print(len(rest_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agent['agent_name_1']= df_agent['person_name']\n",
    "df_agent['agent_national_code_1']= df_agent['person_national_code']\n",
    "df_agent['agent_id_1']= df_agent['person_id']\n",
    "df_agent= df+agent.drop(columns=['person_name','person_national_code','person_id'])\n",
    "df_agent['person_name']= df_agent['agent_name']\n",
    "df_agent['person_id']= df_agent['agent_id']\n",
    "df_agent['person_national_code']= df_agent['agent_national_code']\n",
    "df_agent= df_agent.drop(columns=['agent_name','agent_id','agent_national_code'])\n",
    "df_agent= df_agent.rename(columns={'agent_name_1':'agent_name','agent_national_code_1':'agent_national_code','agent_id_1':'agent_id'})\n",
    "df_agent= df_agent[['company_id','company_national_code','company_name','date','charge_id','charge','person_name','person_id','person_national_code','agent_id','agent_national_code','agent_name','case_national_code']]\n",
    "df_agent['person_type']=2\n",
    "print(f'df_agent : {list(df_agent.columns)} , {len(list(df_agent.columns))}')\n",
    "df_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grapg= df_copy.loc[rest_index]\n",
    "print(f' df_graph: {list(df_graph.columns)}, {len(list(df_graph.columns))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph= df_graph.append(df_agent).resrt_index().drop(columns=['index'])\n",
    "print(f'df_graph: {list(df_graph.columns)} , {len(df_graph.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_graph_df= len(df_graph)\n",
    "len_graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_graph) == len(df_copy):\n",
    "    print('program complete')\n",
    "else:\n",
    "    print('there is problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['date']= df_graph['date'].apply(lambda x: x.replace('/',''))\n",
    "df_graph['date_timestamp']= date_corrector(df_graph['date'])\n",
    "df_graph['year']= df_graph['date_timestamp'].apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## person name problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### person name with person national code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph= checking_words(df=df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dg_graph) == len_graph_df:\n",
    "    print('program complete')\n",
    "else:\n",
    "    print('there is problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### person name without person national code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_index= df_graph.loc[df_graph.person_national_code=='nan']\n",
    "nan_index= nan_index.loc[nan_index.person_type==2].index.tolist()\n",
    "graph_index= df_graph.index.tolist()\n",
    "rest_index= list(set(graph_index)-set(nan_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'length of data without person national code: {len(nan_index)}')\n",
    "print(f'length of data with person national code: {len(rest_index)}')\n",
    "print(f'length of main data: {len(graph_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_nan= df_graph.loc[nan_index]\n",
    "df_graph_nan= insert_code(df_graph_nan, 'person_national_code','person_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_non_nan= df_graph.loc[rest_index]\n",
    "df_graph= df_graph_non_nan.append(df_graph_nan).reset_index().drop(colmns=['index'])\n",
    "print(f'df_graph: {list(df_graph.column)} \\nnumber of column is: {len(list(df_graph.column))} \\nlength df_graph is: {len(df_graph)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph= checking_words(df=df_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some person name that doesnt have national code is remain\n",
    "for this group should set random (rang) national number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_id= df_graph.loc[df_graph['person_national_code']=='nan']\n",
    "person_without_id= list(dict.fromkeys(df_without_id['person_name']))\n",
    "len_without_id= len(person_without_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_range= list(range(1, lem_without_id + 1))\n",
    "len(id_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= 0\n",
    "for i in person_without_id:\n",
    "    df_graph['person_national_code']= df_graph['person_national_code'].nask(df_graph['person_name']==i, str(id_range[0]))\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking company name with person name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_company_person(df=df_graph, national_company_series='comapny_national_code', national_person_series='person_national_code',company_name_series='company_name',person_name_series='persona_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_graph.itterwos():\n",
    "    cursor.execute('''\n",
    "                INSERT INTO table_name (columns name)\n",
    "                VALUES (?)\n",
    "                ''',\n",
    "                row.loc['column name']\n",
    "                )\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_year= int(input(f'select year from{list(dict.fromkeys(df_graph.year))}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_year= df_graph.loc[df_graph.year > selected_year].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict.fromkeys(df_graph_year.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph_year['tuple_edge']= list(df_graph_year[['person_national_code','company_national_code']].itertuple(index=False, name=None))\n",
    "edge_list= list(df_graph_year['tuple_edge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making node dataframe and list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source= df_graph_year[['person_name','person_national_code','person_type']]\n",
    "df_target= df_graph_year[['company_name','company_national_code']]\n",
    "df_source.rename(columns={'person_name':'name','person_national_code':'national_coed','person_type':'type'}, inplace=True)\n",
    "df_target.rename(columns={'company_name':'name','company_national_code':'national_code'}, inplace=True)\n",
    "df_target['type']=2\n",
    "df_node= df_source.append(df_target).reset_index().drop(columns=['index'])\n",
    "df_node= df_node.reset_index()\n",
    "df_node= df_node.drop_duplicates('national_code').reset_index().drop(columns=['level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list= list(dict.fromkeys(df_node.national_code))\n",
    "len(node_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_year= nx.DiGraph()\n",
    "G_year.add_nodes_from(node_list)\n",
    "G_year.add_edges_from(edge_list)\n",
    "directed_pos= nx.spring_layout(G_year, k=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len edge: {len(G_year.edges())}')\n",
    "print(f'len node: {len(G_year.nodes())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name= list(df_node.name)\n",
    "node_type= list(df_node.type)\n",
    "print(f'length of node name\" {len(node_name)}')\n",
    "print(f'length of node type: {len(node_type)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id in G_year.nodes:\n",
    "    node_index= df_node.loc[df_node['national_code']==node_id].index.tolist()\n",
    "    node_index= node_index[0]\n",
    "    G_year.nodes[node_id]['name']=node_name[node_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id in G_year.nodes:\n",
    "    node_index= df_node.loc[df_node['national_code']==node_id].index.tolist()\n",
    "    node_index= node_index[0]\n",
    "    G_year.nodes[node_id]['typr']=node_type[node_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color=[ \n",
    "    '#bb7a44' if G_year.nodes[v]['type']==2\n",
    "    else '#33a02c' for v in G_year\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, w in G_year.edges:\n",
    "    if G_year.nodes[v]['type']== G_year.nodes[w]['type']:\n",
    "        G_year.edges[v, w]['company_company']= True\n",
    "    else:\n",
    "        G_year.edges[v, w]['company_company']= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_co= [e for e in G_year.edges if G_year.edges[e]['company_company']]\n",
    "co_per= [e for e in G_year.edges if ~G_year.edges[e]['company_company']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subgraph by attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making subgraph of the nodes that are company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the remain work on graph will be done on the subgraph of company nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_co= ( \n",
    "    node\n",
    "    for node, data\n",
    "    in G_year.nodes(data=True)\n",
    "    if data.get('type')==2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_co= G_year.subgraph(nodes_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' length of G_co graph: {len(G_co.nodes())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_nodes= []\n",
    "remove_nodes= []\n",
    "for nodes in G_co.nodes():\n",
    "    if G_co.degree(nodes)==1:\n",
    "        each_nodes.append(nodes)\n",
    "for i in each_nodes:\n",
    "    for j in list(G_co.out_edge(i)):\n",
    "        if G_co.degree(j[1])==1:\n",
    "            remove_nodes(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= nx.Digraph(G_co)\n",
    "G.remove_nodes_from(remove_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subgraph related to specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_node_code= int(input('insert specific node national code: '))\n",
    "node_related= list(nx.descendants(G, specific_node_code))\n",
    "node_related.append(specific_node_code)\n",
    "node_related= list(dict.fromkeys(node_related))\n",
    "G_node_related= G.subgraph(node_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_node_related_edge = list(G_node_related.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_node_related_df= pd.DataFrame(G_node_related_egde)\n",
    "G_node_related_df.rename(column={0:'source',1:'target'}, inplace=True)\n",
    "G_node_related_df['bank_name']= 'pasargad'\n",
    "G_node_related_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in G_node_related_df.iterrow():\n",
    "    cursor.execute('''\n",
    "                INSERT INTO table_name (columns name)\n",
    "                VALUES (?)\n",
    "                ''',\n",
    "                row.loc['columns name']\n",
    "                )\n",
    "cnsn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_related = nx.get_node_attributes(G_node_related, name='name')\n",
    "node_related_df= pd.DataFrame(list(name.items()))\n",
    "node_related_df['bank_name']='pasargad'\n",
    "node_related_df.rename(columns={0:'id',1:'label'}, inplace=True)\n",
    "node_related_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in node_related_df:\n",
    "    cursor.execute('''\n",
    "                ISERT INTO table_name (columns)\n",
    "                VALUES (?)\n",
    "                ''',\n",
    "                row.loc['column name']\n",
    "                )\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find complement graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_C= nx.complement(G_node_related)\n",
    "nx.is_strongly_connected(G_C)\n",
    "G_C_edge= list(G_C.edges())\n",
    "G_C_df= pd.DataFrame(G_C_edge)\n",
    "G_C_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding connected nodes by layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find connected nodes to specific node during given layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_node_national_code= int(input('onsert national_code for finding connected nodes'))\n",
    "layer_number= int(input('number of layers to find'))\n",
    "tereshold= list(nx.single_source_shortest_path_length(G_node_related, connected_node_national_code, cutoff= layer_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes_tereshold= []\n",
    "for i in tereshold:\n",
    "    name= G_node_related.nodes[i]\n",
    "    list_nodes_tereshold.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes_tereshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### secind method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find connected nodes to specific node by each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_node_national_code= int(input('onsert national_code for finding connected nodes'))\n",
    "layer_number= int(input('number of layers to find'))\n",
    "if __name__ == '__main__':\n",
    "    print(finding_layers_node(G_node_related, connected_node_national_code,layer_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find strongly connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_con= list(nx.strongly_connected_components(G_node_related))\n",
    "list_name=[]\n",
    "c= 0\n",
    "for item in str_con:\n",
    "    if len (item) > 1:\n",
    "        list_n=[]\n",
    "        for j in item:\n",
    "            name= G_bank.nodes[j]['name']\n",
    "            list_n.append(name)\n",
    "        list_name.append(list_n)\n",
    "        G_layer = G_node_related.subgraph(item)\n",
    "        G_layer_edge= list(G_layer.edges())\n",
    "        G_layer_df= pd.DataFrame(G_layer_edge)\n",
    "        G_layer_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')\n",
    "        c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate indexes for specific subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness= nx.betweenness_centrality(G_node_related, normalized=False, endpoints=True)\n",
    "betweenness_sorted= sorted(betweenness.items(), key= lambda x: x[1], reverse= True)[0:10]\n",
    "list_betweenness=[]\n",
    "for i in betweennes_sorted:\n",
    "    print(G_node_related.nodes[i[0]])\n",
    "    list_betweenness.append(G_node_related.nodes[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_df= pd.DataFrame(list_betweenness)\n",
    "betweenness_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this index to finding importatnt edge in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_betweenness= nx.edge_betweenness_centrality(G_node_related, normalized=False, endpoints=True)\n",
    "edge_betweenness_sorted= sorted(edge_betweenness.items(), key= lambda x: x[1], reverse= True)[0:10]\n",
    "edge_betweenness_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_edge_betweenness=[ids for (ids, measurement) in edge_betweenness_sorted]\n",
    "important_edge_betweenness_list=[]\n",
    "for i in important_edge_betweenness:\n",
    "    name_1= G_node_related[i[0]]['name']\n",
    "    name_2= G_node_related[i[1]]['name']\n",
    "    tuple_name=(name_1 , name_2)\n",
    "    important_edge_betweenness_list.append(tuple_name)\n",
    "important_edge_betweenness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_betweenness_df= pd.DataFrame(important_edge_betweenness_list)\n",
    "edge_betweenness_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness= nx.closeness_centrality(G_node_related, normalized=False, endpoints=True)\n",
    "closeness_sorted= sorted(closeness.items(), key= lambda x: x[1], reverse= True)[0:10]\n",
    "list_closeness=[]\n",
    "for i in closeness_sorted:\n",
    "    print(G_node_related.nodes[i[0]])\n",
    "    list_closeness.append(G_node_related.nodes[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_df= pd.DataFrame(list_closeness)\n",
    "closeness_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree= G_node_related.degree()\n",
    "degree_sorted= sorted(degree, key= lambda x: x[1], reverse= True)[0:10]\n",
    "list_degree=[]\n",
    "for i in degree_sorted:\n",
    "    print(G_node_related.nodes[i[0]])\n",
    "    list_degree.append(G_node_related.nodes[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_df= pd.DataFrame(list_degree)\n",
    "degree_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality= (nx.degree_centrality(G_node_related))\n",
    "degree_centrality_sorted= {k: v for k,v in sorted(degree_centrality.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_degree_centrality=[]\n",
    "for i in degree_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_degree_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality_df= pd.DataFrame(list_degree_centrality)\n",
    "degree_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree= G_node_related.out_degree()\n",
    "out_degree_sorted= sorted(out_degree, key= lambda x: x[1], reverse= True)[0:10]\n",
    "list_out_degree=[]\n",
    "for i in out_degree_sorted:\n",
    "    print(G_node_related.nodes[i[0]])\n",
    "    list_out_degree.append(G_node_related.nodes[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree_df= pd.DataFrame(list_out_degree)\n",
    "out_degree_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree_centrality= (nx.out_degree_centrality(G_node_related))\n",
    "out_degree_centrality_sorted= {k: v for k,v in sorted(out_degree_centrality.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_out_degree_centrality=[]\n",
    "for i in out_degree_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_out_degree_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree_centrality_df= pd.DataFrame(list_out_degree_centrality)\n",
    "out_degree_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in_degree= G_node_related.in_degree()\n",
    "in_degree_sorted= sorted(in_degree, key= lambda x: x[1], reverse= True)[0:10]\n",
    "list_in_degree=[]\n",
    "for i in in_degree_sorted:\n",
    "    print(G_node_related.nodes[i[0]])\n",
    "    list_in_degree.append(G_node_related.nodes[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_df= pd.DataFrame(list_in_degree)\n",
    "in_degree_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_centrality= (nx.in_degree_centrality(G_node_related))\n",
    "in_degree_centrality_sorted= {k: v for k,v in sorted(in_degree_centrality.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_in_degree_centrality=[]\n",
    "for i in in_degree_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_in_degree_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_centrality_df= pd.DataFrame(list_in_degree_centrality)\n",
    "in_degree_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### katz centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes the relative influence of a node within a network by measuring the number of the immediate neighbors and also all other nodes in the network that connect to the node under consideration through these immediate neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katz_centrality= nx.katz_centrality(G_node_related)\n",
    "katz_centrality_sorted= {k: v for k,v in sorted(katz_centrality_sorted.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_katz_centrality=[]\n",
    "for i in katz_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_katz_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katz_centrality_df= pd.DataFrame(list_katz_centrality)\n",
    "katz_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the load centrality of a node is the fraction of all shortest paths that pass through that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_centrality= nx.load_centrality(G_node_related)\n",
    "load_centrality_sorted= {k: v for k,v in sorted(load_centrality.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_load_centrality=[]\n",
    "for i in load_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_load_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_centrality_df= pd.DataFrame(list_load_centrality)\n",
    "load_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### harmonic centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the sum of the reciprocal of the shortest path distances from all other nodes to the specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_centrality= nx.harmonic_centrality(G_node_related)\n",
    "harmonic_centrality_sorted= {k: v for k,v in sorted(harmonic_centrality.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_harmonic_centrality=[]\n",
    "for i in harmonic_centrality_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_harmonic_centrality.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_centrality_df= pd.DataFrame(list_harmonic_centrality)\n",
    "harmonic_centrality_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local reaching centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the local reaching centrality of a node in a directed graph is the proportion of other nodes reachable from that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_national_code= int(input('insert national code for calculate local reaching centrality for: '))\n",
    "nx.local_reaching_centrality(G_node_related, selected_national_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global reaching centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the global reaching centrality of weighted directed graph is the average over all nodes of the difference between the local reaching centrality of the node and the gratest local reaching centrality of any node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.global_reaching_centrality(G_node_related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average neighbor degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_neighbor_degree= nx.average_neighbor_degree(G_node_related)\n",
    "average_neighbor_degree_sorted= {k: v for k,v in sorted(average_neighbor_degree.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_average_neighbor_degree=[]\n",
    "for i in average_neighbor_degree_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_average_neighbor_degree.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_neighbor_degree_df= pd.DataFrame(list_average_neighbor_degree)\n",
    "average_neighbor_degree_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trophic levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trophic_levels= nx.trophic_levels(G_node_related)\n",
    "trophic_levelssorted= {k: v for k,v in sorted(trophic_levels.items(), key= lambda item: item[1], reverse=True)[:10]}\n",
    "list_trophic_levels=[]\n",
    "for i in trophic_levels_sorted:\n",
    "    print(G_node_related.nodes[i])\n",
    "    list_trophic_levels.append(G_node_related.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trophic_levels_df= pd.DataFrame(list_trophic_levels)\n",
    "trophic_levels_df.to_csv('saveing file adress', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trophic differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.trophic_differences(G_node_related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trophic incoherence parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.trophic_incoherence_parameter(G_node_related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voterank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes a ranking of the nodes in a graph based on a voting scheme. with voterank all nodes vote for each of its in neighbours and the node with the highest wotes is selected iteratively. the voting ability of out neighbours of selected nodes is decreased in subsquent turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voterank_list= nx.voterank(G_node_related)\n",
    "vote_rank_name=[]\n",
    "for i in voterank_list:\n",
    "    name= G_node_related.nodes[i]['name']\n",
    "    vote_rank_name.append(name)\n",
    "vote_rank_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eccentricity: ', nx.eccentricity(G_node_related))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Diameter: ', nx.diameter(G_node_related))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Radius: ', nx.radius(G_node_related))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preiphery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preiphery: ', nx.periphery(G_node_related))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Center: ', nx.center(G_node_related))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b6dbfc68184f998ca3ee3e5349d365e100780b212775c518b5644fdf1d4ec58"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
